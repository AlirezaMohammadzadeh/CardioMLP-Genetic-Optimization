# مقدمه
بیماری‌های قلبی عروقی (Cardiovascular Diseases - CVD) به یکی از مهم‌ترین بیماری‌ها در سراسر جهان تبدیل شده‌اند و بالاترین نرخ مرگ و میر و ناتوانی را به خود اختصاص داده‌اند، بدون توجه به اینکه کشورها توسعه‌یافته هستند یا خیر. بر اساس گزارش سازمان بهداشت جهانی، CVD سالانه باعث مرگ حدود 17.9 میلیون نفر می‌شود. به دلیل روند سریع و وخیم شدن این بیماری، بررسی عوامل خطر آن پیچیده شده است. مطالعات بسیاری به صورت آزمایشگاهی (in vitro)، درون‌بدنی (in vivo) و شبیه‌سازی (in silico) بر روی عوامل خطر CVD انجام شده است. این مطالعات نشان داده‌اند که فشار خون سیستولیک و دیاستولیک با عواملی مانند سیگار کشیدن، پیری، زیرگروه‌های وزنی، کلسترول، سطح گلوکز و مصرف الکل مرتبط هستند. به دلیل فرآیند پیچیده و گسترده عوامل خطر، تشخیص زودهنگام CVD و مرحله درمان آن به یک ضرورت تبدیل شده است.

در مطالعات اخیر، تکنیک‌های یادگیری ماشین (Machine Learning - ML) که می‌توانند تعاملات پیچیده عوامل مرتبط با بیماری را توضیح دهند، به طور فزاینده‌ای در مدل‌های پیش‌بینی استفاده می‌شوند. مطالعات بسیاری با استفاده از روش‌های ML و یادگیری عمیق (Deep Learning - DL) انجام شده است. به عنوان مثال، Al'Aref و همکاران در سال 2018 با استفاده از روش‌های تصویربرداری غیرتهاجمی به مطالعات CVD کمک کردند، Ghosh و همکاران در سال 2021 از مدل‌های ML استفاده کردند، و Aryal و همکاران در سال 2020 از روش‌های مبتنی بر میکروبیوم روده استفاده کردند.

روش‌های هوش مصنوعی قابل توضیح (Explainable Artificial Intelligence - XAI) به عنوان راه‌حلی برای نگرانی‌ها درباره قابلیت توضیح مدل و الگوریتم‌های هوش مصنوعی ظهور کرده‌اند. هوش مصنوعی قابل توضیح اخیراً برای کمک به متخصصان بهداشت و درمان در درک روابط بین مدل‌های ML و نتایج پیش‌بینی‌شده آن‌ها استفاده شده است. در عین حال، XAI می‌تواند توانایی تفسیر الگوریتم‌ها را بهبود بخشد و دقت پیش‌بینی مدل‌های پیچیده ML را حفظ کند. در میان مطالعات مربوط به عوامل خطر CVD، مطالعات XAI کمیاب هستند. در این مطالعه، انتخاب اهمیت ویژگی و روش‌های XAI بر روی دیتاست حاوی عوامل خطر CVD استفاده شد. ما اهمیت غالب عوامل خطر بر CVD را با روش استاندارد اهمیت، روش اهمیت ویژگی جایگزین (Permutation Feature Importance - PFI) و روش XAI مقایسه کردیم. با استفاده از روش توضیحات افزایشی شاپلی (Shapley Additive Explanations - SHAP)، عوامل خطری که به طور قابل توجهی بر CVD تأثیر می‌گذارند را شناسایی کردیم. ما روابط بین CVD و عوامل خطر را با نمودارهای مختلف XAI آشکار کردیم و دقت مدل قابل توضیح خود را با مقایسه آن با داده‌های ادبیات اثبات کردیم.

هدف این تحقیق بررسی و مقایسه الگوریتم‌های مختلف یادگیری ماشین و یادگیری عمیق برای پیش‌بینی CVD است.

# درباره دیتاست
این دیتاست که شامل اطلاعات دقیقی درباره عوامل خطر CVD است، با جستجوی "Exploring Risk Factors for Cardiovascular Disease in Adults" در سایت Kaggle به دست آمده است. همچنین، این دیتاست در آدرس https://data.world/kudem در دسترس است. 

دیتاست شامل 70,000 نمونه با 13 ویژگی است که به شرح زیر می‌باشند:
- id: شناسه یکتای هر نمونه (int64)
- age: سن فرد (int64)
- gender: جنسیت فرد (int64)
- height: قد فرد (int64)
- weight: وزن فرد (float64)
- ap_hi: فشار خون سیستولیک (int64)
- ap_lo: فشار خون دیاستولیک (int64)
- cholesterol: سطح کلسترول (int64)
- gluc: سطح گلوکز (int64)
- smoke: وضعیت مصرف سیگار (int64)
- alco: وضعیت مصرف الکل (int64)
- active: میزان فعالیت فیزیکی (int64)
- cardio: وضعیت بیماری قلبی عروقی (int64)

تمامی ستون‌ها بدون مقدار null هستند و حجم کل دیتاست حدود 6.9 مگابایت است. این دیتاست به عنوان منبعی عالی برای محققان در جهت ادغام تکنیک‌های مختلف ML، DL و AI برای کشف روابط بالقوه بین عوامل خطر CVD و CVD دیده می‌شود که در نهایت می‌تواند به درک بهتر عوامل خطر CVD و طراحی اقدامات پیشگیرانه بهتر منجر شود. به همین دلیل، این دیتاست به عنوان یک قطب‌نما برای تعیین گزینه‌های درمانی مناسب به دلیل تشخیص زودهنگام CVD استفاده شده است.

# انتخاب ویژگی (Feature Selection)
روش اهمیت ویژگی جایگزین (Permutation Feature Importance) توضیح می‌دهد که کدام ویژگی‌ها عملکرد و امتیاز مدل را با اندازه‌گیری کاهش عملکرد مدل پس از مخلوط کردن مقادیر یک ویژگی منفرد گمراه می‌کنند. این روش مستقل به ما می‌گوید که مدل چقدر به هر ویژگی وابسته است نسبت به کاهش امتیاز مدل. در این مطالعه، تابع `permutation_importance` برای نشان دادن اهمیت هر ویژگی در دیتاست که بر CVD تأثیر می‌گذارد، استفاده شده است.

روش توضیحات افزایشی شاپلی (Shapley Additive Explanations) ابتدا در نظریه بازی برای تعیین سهم بازیکنان در بازی استفاده شد. این الگوریتم راهی عالی برای ایجاد هم‌افزایی با مهندسی یک مدل جعبه سیاه است. هنگامی که با الگوریتم‌های ML لوندبرگ و لی ترکیب می‌شود، این چارچوب که با مقادیر شاپلی کار می‌کند، شفافیت، قابلیت توضیح و پیش‌بینی‌پذیری را به مدل‌های جعبه سیاه آورد. SHAP بر روی تمام ترکیبات ممکن از ویژگی‌ها کار می‌کند و میانگین سهم‌ها را محاسبه می‌کند. این مطالعه طراحی توضیحی با الگوریتم‌های SHAP برای عوامل خطر CVD ارائه می‌دهد. اگرچه مطالعات زیادی در مورد اهمیت عوامل خطر CVD انجام شده است، این مطالعات ممکن است برای رسیدن به تعاملات بین عوامل برای خروجی‌های مدل کافی نباشند. به همین دلیل، ادغام XAI با استفاده از دیتاست حاوی بسیاری از عوامل خطر انجام شد. ما نمودارهای نیروی فردی و جمعی را برای درک چگونگی تأثیر متغیرهای عوامل خطر بر نتایج مدل بررسی کردیم. سپس، یافته‌های TreeSHAP را با مطالعات قبلی که عامل علّی CVD و تأثیر عوامل خطر بر CVD را پیشنهاد می‌کنند، مقایسه کردیم.

# الگوریتم‌های کلاسیک ماشین لرنینگ
## درخت تصمیم (Decision Tree)
درخت تصمیم یک مدل یادگیری ماشین است که به‌طور گسترده برای مسائل طبقه‌بندی و رگرسیون استفاده می‌شود. این درخت به‌طور گرافیکی نمایش داده می‌شود و از گره‌ها و شاخه‌ها تشکیل می‌شود که به‌وسیله آن‌ها می‌توان تصمیمات مختلف را بر اساس ویژگی‌های ورودی اتخاذ کرد. در اینجا گزارشی جامع از درخت تصمیم آورده شده است:

1. **مفاهیم پایه‌ای درخت تصمیم**
   - درخت تصمیم شامل مجموعه‌ای از گره‌ها و شاخه‌ها است. هر گره نمایانگر یک ویژگی (ویژگی ورودی) است که بر اساس آن تصمیم‌گیری انجام می‌شود. هر شاخه نشان‌دهنده نتیجه یک تصمیم یا دسته‌بندی بر اساس مقدار ویژگی‌ها است.
   - **گره ریشه (Root Node):** اولین گره درخت است که در آن تصمیم‌گیری آغاز می‌شود.
   - **گره‌های داخلی (Internal Nodes):** گره‌هایی که ویژگی‌های ورودی را بررسی کرده و داده‌ها را به گره‌های بعدی تقسیم می‌کنند.
   - **گره‌های برگ (Leaf Nodes):** گره‌هایی که در آن‌ها نتیجه نهایی تصمیم‌گیری (کلاس یا مقدار پیش‌بینی شده) نمایش داده می‌شود.

2. **عملکرد درخت تصمیم**
   - درخت تصمیم به‌طور مداوم داده‌ها را بر اساس ویژگی‌های مختلف تقسیم می‌کند تا در نهایت به یک تصمیم یا پیش‌بینی نهایی برسد. هر تقسیم‌بندی (Split) به‌وسیله یک ویژگی خاص صورت می‌گیرد که داده‌ها را به بهترین نحو ممکن به دو بخش تقسیم می‌کند.

3. **معیارهای تقسیم‌بندی**
   - برای انتخاب ویژگی که داده‌ها را تقسیم کند، از معیارهایی مانند زیر استفاده می‌شود:
     - **گینی (Gini Index):** معیاری است که میزان عدم خلوص داده‌ها را اندازه‌گیری می‌کند. هدف این است که در هر تقسیم، داده‌ها به‌گونه‌ای تقسیم شوند که هر دو بخش به‌طور قابل توجهی متفاوت باشند.
     - **انتروپی (Entropy):** از تئوری اطلاعات برای اندازه‌گیری پیچیدگی یا ناهمگنی داده‌ها استفاده می‌شود. هر چه مقدار انتروپی کمتر باشد، گره‌ها خالص‌تر هستند.
     - **آمار خطای معیار (Mean Squared Error یا MSE):** در مسائل رگرسیون برای کاهش خطای پیش‌بینی استفاده می‌شود.

4. **مزایای درخت تصمیم**
   - **تفسیرپذیری ساده:** درخت تصمیم به‌صورت گرافیکی نمایش داده می‌شود و این باعث می‌شود که تفسیر و توضیح روند تصمیم‌گیری برای انسان‌ها ساده باشد.
   - **قابلیت استفاده برای داده‌های عددی و دسته‌ای:** درخت تصمیم می‌تواند هم برای داده‌های عددی و هم داده‌های دسته‌ای استفاده شود.
   - **خودکار بودن فرآیند انتخاب ویژگی‌ها:** درخت تصمیم به‌طور خودکار ویژگی‌های مهم‌تر را برای تقسیم‌بندی داده‌ها شناسایی می‌کند.

5. **معایب درخت تصمیم**
   - **مستعد به overfitting:** اگر درخت تصمیم خیلی بزرگ باشد و بیش از حد پیچیده شود، ممکن است به داده‌های آموزشی خاص تطبیق پیدا کند و نتواند به‌خوبی بر روی داده‌های جدید پیش‌بینی کند.
   - **عدم کارایی در مسائل پیچیده:** درخت‌های تصمیمی که به‌صورت مستقیم ایجاد می‌شوند، معمولاً در مسائل پیچیده با تعداد زیاد ویژگی‌ها و داده‌های زیاد دقت کمتری دارند.

6. **راه‌کارهای بهینه‌سازی درخت تصمیم**
   - برای جلوگیری از مشکلاتی مانند overfitting و پیچیدگی زیاد، می‌توان تکنیک‌های مختلفی را به کار برد:
     - **کاهش عمق درخت (Pruning):** با محدود کردن عمق درخت، می‌توان از پیچیدگی بیش از حد جلوگیری کرد.
     - **تصادفی‌سازی درخت‌ها (Random Forest):** یکی از تکنیک‌های معروف برای بهبود عملکرد درخت‌های تصمیم، استفاده از جنگل تصادفی است که مجموعه‌ای از درخت‌های تصمیم را به‌طور همزمان می‌سازد و تصمیم‌گیری را بر اساس اکثریت آرا انجام می‌دهد.

7. **استفاده‌های معمول درخت تصمیم**
   - درخت‌های تصمیم برای انواع مسائل به کار می‌روند:
     - **طبقه‌بندی:** درخت‌های تصمیم برای مسائل طبقه‌بندی مانند شناسایی ایمیل‌های اسپم یا دسته‌بندی مشتریان بر اساس رفتار خرید استفاده می‌شوند.
     - **رگرسیون:** در اینجا درخت تصمیم برای پیش‌بینی مقدار عددی (مانند پیش‌بینی قیمت خانه) به کار می‌رود.
     - **تشخیص الگو:** در برخی مسائل شبیه‌سازی، درخت‌های تصمیم به تشخیص الگوهای خاص یا رفتارهای خاص در داده‌ها کمک می‌کنند.

8. **مثال**
   - برای مثال، فرض کنید می‌خواهیم یک درخت تصمیم برای پیش‌بینی این که آیا یک فرد می‌تواند وام دریافت کند یا نه بسازیم. ویژگی‌هایی مانند درآمد فرد، سابقه اعتباری، و تعداد وام‌های پیشین به‌عنوان ویژگی‌ها در نظر گرفته می‌شوند. درخت تصمیم ممکن است به‌صورت زیر باشد:
     - **گره ریشه:** درآمد فرد
       - اگر درآمد بالاتر از یک مقدار مشخص باشد، به گره بعدی با بررسی سابقه اعتباری می‌رود.
       - اگر درآمد کمتر از مقدار مشخص باشد، ممکن است تصمیم به رد درخواست گرفته شود.

## جنگل تصادفی (Random Forest)
جنگل تصادفی یک الگوریتم یادگیری ماشین مبتنی بر روش‌های یادگیری گروهی (Ensemble Learning) است که برای طبقه‌بندی و پیش‌بینی داده‌ها استفاده می‌شود. این الگوریتم از چندین درخت تصمیم‌گیری تشکیل شده است که هرکدام به طور مستقل مدل‌هایی برای پیش‌بینی تولید می‌کنند و پیش‌بینی‌های این درخت‌ها با یکدیگر ترکیب می‌شود تا دقت و کارایی مدل بهبود یابد.

1. **چگونه جنگل تصادفی کار می‌کند؟**
   - **درختان تصمیم‌گیری:** جنگل تصادفی از مجموعه‌ای از درختان تصمیم‌گیری تشکیل می‌شود که هر درخت مدل مستقلی از داده‌ها ایجاد می‌کنند. درختان تصمیم‌گیری از ویژگی‌ها برای تصمیم‌گیری استفاده می‌کنند.
   - **روش Bootstrap Aggregating (Bagging):** در این روش، نمونه‌هایی از داده‌ها به طور تصادفی از مجموعه آموزشی انتخاب می‌شوند و هر درخت با یک زیرمجموعه تصادفی از داده‌ها ساخته می‌شود. این روش از overfitting جلوگیری می‌کند.
   - **انتخاب تصادفی ویژگی‌ها:** هنگام ساخت درخت‌ها، ویژگی‌ها به صورت تصادفی انتخاب می‌شوند تا درخت‌ها از یکدیگر متفاوت باشند و مدل تنوع بالاتری داشته باشد.

2. **مزایای جنگل تصادفی**
   - **دقت بالا:** ترکیب مدل‌ها باعث افزایش دقت پیش‌بینی‌ها می‌شود.
   - **مقاومت در برابر overfitting:** به دلیل استفاده از چندین مدل مختلف، احتمال overfitting کاهش می‌یابد.
   - **مقیاس‌پذیری:** قادر است با داده‌های بزرگ به خوبی کار کند.
   - **کار با داده‌های گمشده:** می‌تواند داده‌های ناقص را به خوبی پردازش کند.
   - **تشخیص ویژگی‌های مهم:** این الگوریتم قادر به شناسایی ویژگی‌های موثر در پیش‌بینی‌هاست.

3. **معایب جنگل تصادفی**
   - **توضیح‌پذیری کم:** مدل‌های پیچیده جنگل تصادفی به راحتی قابل تفسیر نیستند.
   - **هزینه محاسباتی:** به دلیل تعداد زیاد درخت‌ها، نیاز به منابع محاسباتی بیشتری دارد.
   - **کاهش کارایی با ویژگی‌های بی‌اهمیت:** وجود ویژگی‌های بی‌اهمیت می‌تواند دقت مدل را کاهش دهد.

4. **پارامترهای کلیدی**
   - **n_estimators:** تعداد درختان در جنگل تصادفی.
   - **max_depth:** عمق هر درخت تصمیم‌گیری.
   - **min_samples_split:** حداقل تعداد نمونه‌ها برای تقسیم داده‌ها.
   - **max_features:** تعداد ویژگی‌هایی که در هر گره انتخاب می‌شود.

5. **کاربردها**
   - **طبقه‌بندی تصاویر**
   - **پیش‌بینی قیمت‌ها**
   - **شبیه‌سازی‌های پزشکی**
   - **شناسایی تقلب**
   - **تحلیل داده‌های ژنتیک**

## XGBoost
XGBoost (Extreme Gradient Boosting) یک الگوریتم یادگیری ماشین است که به منظور انجام پیش‌بینی‌های دقیق در مسائل رگرسیون و طبقه‌بندی طراحی شده است. این الگوریتم به طور ویژه به دلیل کارایی بالا، سرعت و دقت در رقابت‌های مختلف یادگیری ماشین مشهور است.

XGBoost از تکنیک‌های بازیابی گرادیان تقویتی (Gradient Boosting) استفاده می‌کند. در این روش، مدل‌ها به طور سریالی ساخته می‌شوند و هر مدل جدید به گونه‌ای ساخته می‌شود که خطای مدل‌های قبلی را اصلاح کند.

1. **چرا XGBoost محبوب است؟**
   - **سرعت بالا:** XGBoost بسیار سریع‌تر از بسیاری از الگوریتم‌های مشابه مانند Random Forest و AdaBoost است.
   - **دقت بالا:** این الگوریتم در بسیاری از رقابت‌های یادگیری ماشین، از جمله Kaggle، مقام‌های بالایی را کسب کرده است.
   - **قابلیت تنظیم:** XGBoost قابلیت تنظیم زیادی برای بهبود عملکرد مدل دارد.
   - **مدیریت داده‌های گمشده:** XGBoost به طور خودکار می‌تواند داده‌های گمشده را مدیریت کند و نیازی به پیش‌پردازش داده‌ها برای این موضوع نیست.
   - **مقاومت در برابر Overfitting:** به دلیل استفاده از regularization (تنظیم) L1 و L2، این الگوریتم از overfitting جلوگیری می‌کند.

2. **اصول پایه‌ای الگوریتم**
   - XGBoost یک الگوریتم مبتنی بر درخت تصمیم‌گیری (Decision Trees) است که به صورت Gradient Boosting عمل می‌کند. در روش Gradient Boosting، مدل‌ها به طور سلسله‌وار ساخته می‌شوند و هر مدل جدید سعی دارد خطای مدل قبلی را کاهش دهد.
   - **Gradient Boosting:** به هر درخت تصمیم‌گیری وزن‌هایی تخصیص داده می‌شود و مدل سعی می‌کند تا با استفاده از این درخت‌ها خطای پیش‌بینی را به حداقل برساند. این فرآیند به طور تکراری انجام می‌شود تا دقت مدل افزایش یابد.
   - **Regularization:** XGBoost از تکنیک‌هایی مانند L1 و L2 regularization برای جلوگیری از overfitting استفاده می‌کند. این باعث می‌شود که مدل‌های XGBoost عملکرد بهتری در مواجهه با داده‌های پیچیده و پر نویز داشته باشند.

3. **اجزای اصلی XGBoost**
   - **Booster Type:** نوع تقویت‌کننده (Boosting) انتخابی است. XGBoost از دو نوع تقویت‌کننده پشتیبانی می‌کند:
     - **GBTree:** استفاده از درخت‌های تصمیم‌گیری.
     - **GBLinear:** استفاده از مدل‌های خطی.
   - **Objective Function:** این تابع برای اندازه‌گیری کیفیت مدل استفاده می‌شود و هدف اصلی در یادگیری ماشین معمولاً به حداقل رساندن خطا است.
   - **Evaluation Metrics:** XGBoost امکان استفاده از انواع معیارهای ارزیابی مانند خطای میانگین مربعات (RMSE)، دقت (Accuracy) و سایر معیارها را فراهم می‌کند.

4. **ویژگی‌های اصلی XGBoost**
   - **Parallel Processing:** یکی از ویژگی‌های منحصر به فرد XGBoost پردازش موازی است که باعث می‌شود مدل به طور قابل توجهی سریع‌تر از سایر الگوریتم‌ها آموزش ببیند.
   - **Handling Missing Data:** XGBoost به طور خودکار داده‌های گمشده را مدیریت می‌کند و در فرآیند آموزش تاثیری منفی نمی‌گذارد.
   - **Tree Pruning:** الگوریتم به صورت خودکار درخت‌های تصمیم‌گیری را هرس می‌کند تا مدل از overfitting جلوگیری کند.

5. **فرآیند آموزش XGBoost**
   - آموزش مدل در XGBoost از سه بخش اصلی تشکیل می‌شود:
     - **Initial Model:** ابتدا یک مدل اولیه ساخته می‌شود.
     - **Boosting Iterations:** در هر تکرار، یک درخت جدید ساخته می‌شود که هدف آن اصلاح خطاهای مدل قبلی است.
     - **Model Optimization:** در نهایت، با استفاده از تکنیک‌های regularization و تنظیمات دقیق، مدل بهینه‌سازی می‌شود.

6. **تنظیمات و پارامترهای XGBoost**
   - XGBoost تعداد زیادی پارامتر قابل تنظیم دارد که می‌تواند به بهبود عملکرد مدل کمک کند. از مهم‌ترین آن‌ها می‌توان به موارد زیر اشاره کرد:
     - **learning_rate (eta):** نرخ یادگیری که میزان تاثیر هر درخت را در مدل تعیین می‌کند.
     - **n_estimators:** تعداد درخت‌ها (تعداد تکرارهای تقویت‌کننده).
     - **max_depth:** عمق هر درخت.
     - **subsample:** درصد نمونه‌هایی که برای ساخت هر درخت انتخاب می‌شوند.
     - **colsample_bytree:** درصد ویژگی‌هایی که برای هر درخت استفاده می‌شوند.
     - **lambda (L2 regularization):** میزان جریمه برای جلوگیری از پیچیدگی مدل.
     - **alpha (L1 regularization):** جریمه L1 برای مدل.

7. **کاربردهای XGBoost**
   - XGBoost در بسیاری از کاربردهای یادگیری ماشین مورد استفاده قرار می‌گیرد، از جمله:
     - **طبقه‌بندی:** پیش‌بینی دسته‌بندی داده‌ها (مثلاً شناسایی ایمیل‌های اسپم).
     - **رگرسیون:** پیش‌بینی مقادیر عددی (مثلاً پیش‌بینی قیمت خانه‌ها).
     - **مسائل رتبه‌بندی:** استفاده در مسائلی مانند موتور جستجو.
     - **کلاس‌بندی چندکلاسه:** استفاده در مسائل با بیش از دو کلاس.

8. **مزایا و معایب**
   - **مزایا:**
     - سرعت بالا و مقیاس‌پذیری.
     - دقت بالا در پیش‌بینی‌ها.
     - قابلیت تنظیم و سفارشی‌سازی.
     - مقاومت در برابر داده‌های گمشده و نویز.
   - **معایب:**
     - پیچیدگی تنظیمات و پارامترها.
     - نیاز به حافظه زیاد برای داده‌های بزرگ.
     - ممکن است در برخی مسائل ساده‌تر از مدل‌های دیگر مانند Logistic Regression یا SVM بهتر عمل نکند.

## KNeighbor Classifier
الگوریتم K-Nearest Neighbors (KNN) یک الگوریتم یادگیری نظارت‌شده است که برای مسائل طبقه‌بندی و رگرسیون استفاده می‌شود. در این الگوریتم، برای پیش‌بینی یک نمونه جدید، نزدیک‌ترین نقاط داده به آن نمونه انتخاب می‌شوند و بر اساس آن‌ها پیش‌بینی انجام می‌گیرد. در ادامه به توضیح اجزای اصلی این الگوریتم و نحوه عملکرد آن پرداخته شده است:

1. **اصول کلی الگوریتم KNN:**
   - در ابتدا، تمامی داده‌های آموزشی به صورت یک مجموعه از نقاط داده در فضای n بعدی ذخیره می‌شوند.
   - برای پیش‌بینی یک نمونه جدید، الگوریتم فاصله آن نمونه را از تمامی نمونه‌های موجود در مجموعه آموزشی محاسبه می‌کند.
   - از آنجایی که این الگوریتم بر اساس مفهوم "نزدیکی" عمل می‌کند، معمولاً از فاصله‌های مختلفی مانند فاصله اقلیدسی یا فاصله مانهاتن برای محاسبه نزدیکی استفاده می‌شود.
   - سپس، K نزدیک‌ترین نمونه‌ها به نمونه جدید انتخاب می‌شوند.
   - در نهایت، بر اساس بیشترین تعداد نمونه‌ها از یک کلاس خاص، کلاس پیش‌بینی می‌شود. این یعنی کلاس نمونه جدید به طور معمول مشابه‌ترین نمونه‌ها خواهد بود.

2. **انتخاب مقدار K:**
   - K تعداد همسایگان نزدیک است که باید برای پیش‌بینی استفاده شوند. انتخاب K مناسب نقش بسیار مهمی در دقت الگوریتم دارد.
     - اگر K بسیار کوچک باشد، ممکن است الگوریتم به شدت به نویز حساس شود و پیش‌بینی‌های غلط بدهد.
     - اگر K بسیار بزرگ باشد، ممکن است الگوریتم قادر به شبیه‌سازی پیچیدگی داده‌ها نباشد و منجر به مدل‌سازی بیش از حد ساده شود.
   - معمولاً از روش‌هایی مانند اعتبارسنجی متقابل (Cross-validation) برای انتخاب مقدار بهینه K استفاده می‌شود.

3. **محاسبه فاصله:**
   - در این الگوریتم، برای محاسبه نزدیکی نمونه‌ها، معمولاً از فاصله اقلیدسی استفاده می‌شود، که در آن فاصله بین دو نقطه در فضای n بعدی به صورت زیر محاسبه می‌شود:
     \[
     d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}
     \]
   - این فاصله برای هر جفت نمونه محاسبه می‌شود و سپس K نزدیک‌ترین نقاط به نمونه جدید انتخاب می‌شوند.

4. **مزایا و معایب KNN:**
   - **مزایا:**
     - الگوریتم KNN ساده و مفهومی است.
     - نیاز به آموزش مدل ندارد و به سرعت می‌توان آن را برای داده‌های جدید به کار برد.
     - از آنجا که به صورت غیرپارامتریک عمل می‌کند، توانایی مدل‌سازی داده‌های پیچیده با شکل‌های مختلف را دارد.
   - **معایب:**
     - از آنجا که برای پیش‌بینی باید فاصله تمامی نمونه‌ها محاسبه شود، این الگوریتم در صورتی که داده‌ها زیاد باشند، زمان‌بر خواهد بود (O(n) برای هر پیش‌بینی).
     - حساسیت بالایی به مقیاس ویژگی‌ها دارد، به همین دلیل ممکن است نیاز به نرمال‌سازی داده‌ها باشد.

5. **کاربردها:**
   - **طبقه‌بندی ایمیل‌ها:** استفاده در تشخیص اسپم یا غیر اسپم.
   - **تشخیص دست‌خط:** برای شناسایی اعداد یا حروف نوشته شده.
   - **پیشنهادات محصول:** پیشنهاد محصولات مشابه به کاربران براساس خریدهای گذشته.

در نهایت، الگوریتم KNN یکی از الگوریتم‌های پرکاربرد و ساده در یادگیری ماشین است که در بسیاری از مسائل طبقه‌بندی و رگرسیون استفاده می‌شود. البته انتخاب پارامتر K و نحوه محاسبه فاصله‌ها می‌تواند تأثیر زیادی در کارایی آن داشته باشد.

## SVM Classifier
طبقه‌بند ماشین بردار پشتیبان (SVM) یکی از الگوریتم‌های قدرتمند در یادگیری ماشین است که به‌طور عمده برای مسائل دسته‌بندی و رگرسیون استفاده می‌شود. SVM به‌ویژه در مواجهه با داده‌های با ابعاد بالا و پیچیدگی‌های هندسی کاربرد گسترده‌ای دارد. این الگوریتم توانایی تفکیک داده‌های خطی و غیرخطی را دارد و بر اساس حداکثر کردن حاشیه تصمیم‌گیری، قابلیت تعمیم‌پذیری بالایی را ارائه می‌دهد.

1. **مفاهیم پایه‌ای SVM**
   - **هایپربال (Hyperplane):** در فضای n-بعدی، هایپربال یک مرز یا صفحه‌ای است که داده‌ها را به دو گروه جدا می‌کند. برای داده‌های دو بعدی (۲D)، این هایپربال یک خط است و برای داده‌های سه‌بعدی (۳D)، یک صفحه (plane) خواهد بود. در n-بعدی، این صفحه به‌طور کلی "هایپربال" نامیده می‌شود.
   - **بردارهای پشتیبان (Support Vectors):** بردارهای پشتیبان به نقاط داده گفته می‌شود که در نزدیکی هایپربال قرار دارند و از آن عبور نمی‌کنند. این نقاط حیاتی برای تعیین مکان و موقعیت هایپربال هستند و الگوریتم سعی می‌کند حاشیه بین این نقاط و هایپربال را حداکثر کند.
   - **حاشیه (Margin):** حاشیه به فاصله بین نزدیک‌ترین داده‌ها از هر کلاس به هایپربال گفته می‌شود. SVM تلاش می‌کند حاشیه را حداکثر کند تا تعمیم‌پذیری مدل به بهترین شکل انجام شود.

2. **نحوه عملکرد SVM**
   - هدف اصلی SVM، یافتن هایپربالی است که با بیشترین فاصله، داده‌ها را از دو کلاس مختلف جدا کند.
   - **خطی یا غیرخطی بودن داده‌ها:** اگر داده‌ها به‌طور خطی قابل تفکیک باشند، SVM به‌راحتی یک هایپربال پیدا می‌کند. در غیر این صورت، داده‌ها باید به فضایی با ابعاد بالاتر نقشه‌برداری شوند، جایی که ممکن است تفکیک خطی امکان‌پذیر باشد.
   - **هسته‌ها (Kernels):** برای داده‌های غیرخطی، الگوریتم SVM از روش‌های مختلفی به نام "هسته‌ها" (kernels) برای تبدیل داده‌ها به فضای با ابعاد بالاتر استفاده می‌کند.

3. **هسته‌ها (Kernels) در SVM**
   - **هسته خطی (Linear Kernel):** در این حالت داده‌ها بدون نیاز به تبدیل فضای بالا، مستقیماً در فضای اصلی خود جدا می‌شوند.
   - **هسته رادیال بیس (RBF):** این هسته یکی از محبوب‌ترین هسته‌ها در SVM است و برای داده‌هایی که به‌طور غیرخطی قابل تفکیک هستند، استفاده می‌شود.
   - **هسته چندجمله‌ای (Polynomial Kernel):** این هسته برای داده‌هایی که به‌طور پیچیده‌تری به هم متصل هستند، استفاده می‌شود.
   - **هسته Sigmoid:** این هسته بیشتر در شبکه‌های عصبی استفاده می‌شود اما در SVM نیز به‌کار می‌رود.

4. **مزایا و معایب SVM**
   - **مزایا:**
     - دقت بالا: SVM به‌ویژه در مسائل دسته‌بندی با مرزهای تصمیم‌گیری واضح، دقت بسیار بالایی دارد.
     - تعمیم‌پذیری عالی: SVM با حداکثر کردن حاشیه به‌طور طبیعی از تعمیم‌پذیری بالایی برخوردار است.
     - عملکرد با ابعاد بالا: SVM به‌طور مؤثری با داده‌های با ابعاد بالا کار می‌کند.
     - قابلیت استفاده از هسته‌ها: SVM این امکان را فراهم می‌آورد که داده‌های غیرخطی را به فضای بالاتر تبدیل کرده و تفکیک خطی را ممکن سازد.
   - **معایب:**
     - زمان پردازش طولانی: برای داده‌های بزرگ و پیچیده، آموزش SVM ممکن است زمان‌بر باشد.
     - حساسیت به مقیاس داده‌ها: داده‌ها باید به‌طور دقیق پیش‌پردازش شوند و مقیاس‌بندی شوند.
     - انتخاب هسته مناسب: انتخاب هسته مناسب یکی از چالش‌های اصلی در استفاده از SVM است.

5. **کاربردهای SVM**
   - **دسته‌بندی تصاویر و شناسایی الگو:** SVM در پردازش تصویر برای شناسایی و دسته‌بندی اشیاء، صورت‌ها، و ویژگی‌های خاص تصاویر استفاده می‌شود.
   - **شناسایی دست‌خط و OCR (تشخیص نویسه نوری):** SVM در سیستم‌های OCR برای شناسایی و دسته‌بندی حروف و اعداد از تصاویر استفاده می‌شود.
   - **پزشکی و زیست‌شناسی:** SVM برای شناسایی و طبقه‌بندی انواع مختلف سرطان‌ها، بیماری‌ها و داده‌های ژنتیکی کاربرد دارد.
   - **شناسایی تقلب و امنیت:** در بانکداری، SVM برای شناسایی الگوهای تقلبی در تراکنش‌ها استفاده می‌شود.
   - **مدیریت منابع انسانی:** در حوزه‌های منابع انسانی، SVM می‌تواند به‌عنوان یک ابزار برای پیش‌بینی عملکرد کارکنان به کار رود.
   - **تحلیل متن و تحلیل احساسات:** در پردازش زبان طبیعی (NLP) و تحلیل احساسات، SVM برای دسته‌بندی متون به گروه‌های مثبت، منفی، و خنثی استفاده می‌شود.

## Gaussian NB
الگوریتم Naive Bayes، یک الگوریتم طبقه‌بندی مبتنی بر احتمال است که فرض می‌کند تمام ویژگی‌های ورودی مستقل از یکدیگر هستند. این فرض که ویژگی‌ها مستقل از یکدیگر هستند، باعث می‌شود که محاسبات الگوریتم بسیار ساده و سریع باشد. در حالی که این فرض ممکن است در دنیای واقعی نادرست باشد، اما این الگوریتم با وجود سادگی‌اش عملکرد خوبی در بسیاری از مسائل طبقه‌بندی دارد.

مدل Gaussian Naive Bayes یک نسخه خاص از Naive Bayes است که برای داده‌هایی مناسب است که ویژگی‌های آن‌ها توزیع نرمال (Gaussian) دارند. در این مدل فرض می‌شود که برای هر ویژگی، داده‌ها به صورت توزیع نرمال یا گاوسی پراکنده شده‌اند.

1. **قاعده بیز (Bayes' Theorem)**
   - الگوریتم Naive Bayes بر اساس قاعده بیز ساخته شده است که رابطه‌ای بین احتمال‌های شرطی دارد:
     \[
     P(C∣X)=\frac{P(X∣C)P(C)}{P(X)}
     \]
   - در اینجا:
     - \(P(C∣X)\): احتمال تعلق داده \(X\) به کلاس \(C\) (چیزی که می‌خواهیم پیش‌بینی کنیم).
     - \(P(X∣C)\): احتمال مشاهده ویژگی‌ها \(X\) با فرض اینکه داده به کلاس \(C\) تعلق دارد.
     - \(P(C)\): احتمال پیشین کلاس \(C\) (احتمال اینکه داده به کلاس \(C\) تعلق داشته باشد).
     - \(P(X)\): احتمال مشاهده ویژگی‌های \(X\).

2. **مدل Gaussian Naive Bayes**
   - در Gaussian Naive Bayes، هر ویژگی \(X_i\) برای هر کلاس \(C\) به صورت یک توزیع نرمال مدل‌سازی می‌شود. بنابراین، برای هر ویژگی \(X_i\) در کلاس \(C\)، احتمال شرطی آن به شکل زیر خواهد بود:
     \[
     P(X_i∣C)=\frac{1}{\sqrt{2\pi\sigma_C^2}} \exp\left(-\frac{(X_i-\mu_C)^2}{2\sigma_C^2}\right)
     \]
   - که در آن:
     - \(\mu_C\) میانگین داده‌های ویژگی \(X_i\) در کلاس \(C\) است.
     - \(\sigma_C^2\) واریانس داده‌های ویژگی \(X_i\) در کلاس \(C\) است.

3. **فرآیند آموزش و پیش‌بینی**
   - **آموزش (Training Phase):** در فاز آموزش، مدل Gaussian Naive Bayes برای هر کلاس \(C\) باید دو پارامتر اساسی را یاد بگیرد:
     - میانگین \(\mu_C\) برای هر ویژگی \(X_i\) در کلاس \(C\)
     - واریانس \(\sigma_C^2\) برای هر ویژگی \(X_i\) در کلاس \(C\)
   - **پیش‌بینی (Prediction Phase):** پس از آموزش، مدل برای پیش‌بینی کلاس یک نمونه جدید از قاعده بیز استفاده می‌کند. به این صورت که:
     - برای هر کلاس \(C\)، احتمال \(P(C∣X)\) محاسبه می‌شود.
     - این احتمال‌ها با استفاده از قاعده بیز و احتمال‌های شرطی مدل شده (که از توزیع گاوسی استفاده می‌کنند) محاسبه می‌شود.
     - کلاس با بیشترین احتمال \(P(C∣X)\) انتخاب می‌شود.

4. **مزایا و معایب**
   - **مزایا:**
     - ساده و سریع: الگوریتم Gaussian Naive Bayes بسیار ساده است و می‌تواند به سرعت روی داده‌ها اجرا شود.
     - عملکرد خوب در بسیاری از مشکلات: حتی با وجود فرض استقلال ویژگی‌ها، این الگوریتم عملکرد خوبی در بسیاری از مسائل طبقه‌بندی دارد.
     - نیاز به حافظه کم: این الگوریتم نیاز به حافظه کمتری نسبت به بسیاری از مدل‌های پیچیده دیگر دارد.
     - تعامل کم با داده‌های ورودی: به‌طور معمول، نیاز به پیش‌پردازش پیچیده‌ای ندارد.
   - **معایب:**
     - فرض استقلال ویژگی‌ها: بزرگترین ایراد الگوریتم Naive Bayes این است که فرض می‌کند ویژگی‌ها مستقل از یکدیگر هستند، که در بسیاری از موارد درست نیست.
     - حساس به نرمال بودن داده‌ها: چون ویژگی‌ها باید توزیع نرمال داشته باشند، الگوریتم ممکن است در داده‌هایی که توزیع نرمال ندارند دقت کمتری داشته باشد.
     - عدم قابلیت یادگیری روابط پیچیده: به‌خاطر سادگی مدل، قادر به یادگیری روابط پیچیده بین ویژگی‌ها نیست.

5. **کاربردها**
   - Gaussian Naive Bayes در بسیاری از زمینه‌ها مورد استفاده قرار می‌گیرد، از جمله:
     - **طبقه‌بندی متن:** مانند تشخیص اسپم در ایمیل یا تحلیل احساسات.
     - **تشخیص بیماری:** برای طبقه‌بندی بیماران به دسته‌های مختلف با توجه به ویژگی‌های پزشکی.
     - **پردازش تصویر:** در تشخیص الگوهای مختلف در تصاویر.

## Logistic Regression
رگرسیون لجستیک یکی از الگوریتم‌های محبوب در یادگیری ماشین است که به طور خاص در مسائل دسته‌بندی استفاده می‌شود. این مدل برای پیش‌بینی احتمال وقوع یک رویداد استفاده می‌شود و به طور گسترده در مسائل مختلف مانند تشخیص بیماری‌ها، شناسایی تقلب، تحلیل داده‌های بازاریابی و بسیاری از کاربردهای دیگر به کار می‌رود.

در رگرسیون لجستیک، هدف پیش‌بینی متغیر وابسته (که معمولاً دوگانه است) بر اساس مجموعه‌ای از متغیرهای مستقل است. برخلاف رگرسیون خطی که پیش‌بینی‌های پیوسته ارائه می‌دهد، رگرسیون لجستیک برای پیش‌بینی احتمال وقوع یک رویداد بین ۰ و ۱ طراحی شده است.

1. **نحوه عملکرد رگرسیون لجستیک**
   - **مدل ریاضی رگرسیون لجستیک:** رگرسیون لجستیک یک مدل غیرخطی است که از تابع سیگموئید (یا لجستیک) برای تبدیل پیش‌بینی‌های خطی به مقادیر بین ۰ و ۱ استفاده می‌کند. معادله کلی برای مدل رگرسیون لجستیک به صورت زیر است:
     \[
     p(y=1∣X)=\frac{1}{1+e^{-(β_0+β_1X_1+β_2X_2+...+β_nX_n)}}
     \]
   - در اینجا:
     - \(p(y=1∣X)\): احتمال وقوع رویداد کلاس 1 (مثلاً بیماری) برای داده ورودی \(X\).
     - \(X_1, X_2, ..., X_n\): ویژگی‌ها یا متغیرهای ورودی (ویژگی‌های پیش‌بینی‌کننده).
     - \(β_0, β_1, ..., β_n\): ضرایب مدل که از طریق فرآیند آموزش به‌دست می‌آیند.
     - \(e\): پایه لگاریتم طبیعی.

2. **تابع سیگموئید (لجستیک)**
   - تابع سیگموئید به صورت زیر تعریف می‌شود:
     \[
     σ(z)=\frac{1}{1+e^{-z}}
     \]
   - که \(z\) معمولاً به صورت ترکیب خطی ویژگی‌ها \((β_0+β_1X_1+β_2X_2+...)\) است. این تابع خاصیت مهمی دارد که هر مقدار ورودی را به مقداری در بازه \( (0,1) \) تبدیل می‌کند، که به راحتی می‌توان آن را به احتمال تفسیر کرد.

3. **فرآیند آموزش و بهینه‌سازی مدل**
   - **تابع هزینه (Cost Function):** برای بهینه‌سازی مدل، از تابع هزینه استفاده می‌شود که هدف آن کمینه کردن تفاوت پیش‌بینی‌های مدل با مقادیر واقعی است. در رگرسیون لجستیک، از تابع هزینه لگاریتمی (Log Loss) استفاده می‌شود:
     \[
     J(β)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(p^{(i)})+(1-y^{(i)})\log(1-p^{(i)})]
     \]
   - **الگوریتم بهینه‌سازی:** برای بهینه‌سازی تابع هزینه، معمولاً از گرادیان کاهشی (Gradient Descent) استفاده می‌شود. در این روش، ضرایب به صورت تکراری به‌روزرسانی می‌شوند تا زمانی که تابع هزینه به کمترین مقدار ممکن برسد.

4. **مزایا و محدودیت‌های رگرسیون لجستیک**
   - **مزایا:**
     - سادگی و تفسیرپذیری: رگرسیون لجستیک نسبت به بسیاری از مدل‌های پیچیده دیگر ساده‌تر است و نتایج آن قابل تفسیر است.
     - عملکرد خوب در مسائل خطی: این الگوریتم در مسائل دسته‌بندی که رابطه خطی بین ویژگی‌ها و کلاس‌ها وجود دارد، عملکرد خوبی دارد.
     - توانایی پیش‌بینی احتمال: رگرسیون لجستیک به‌طور طبیعی احتمال وقوع یک رویداد را پیش‌بینی می‌کند که در بسیاری از کاربردها مفید است.
     - مقاومت در برابر نویز: به‌ویژه در صورت وجود داده‌های نسبتا تمیز، رگرسیون لجستیک مقاوم به نویز است.
   - **محدودیت‌ها:**
     - فرض خطی بودن رابطه: رگرسیون لجستیک فرض می‌کند که رابطه بین ویژگی‌ها و احتمال وقوع رویداد خطی است. در مواردی که رابطه غیرخطی باشد، این مدل ممکن است عملکرد ضعیفی داشته باشد.
     - حساسیت به داده‌های نامتوازن: اگر توزیع داده‌ها بسیار نامتوازن باشد (مثلاً یک کلاس خیلی نادر است)، مدل ممکن است به سمت پیش‌بینی کلاس غالب تمایل پیدا کند.
     - نیاز به ویژگی‌های مستقل: فرض اساسی رگرسیون لجستیک این است که ویژگی‌ها از هم مستقل باشند. اگر این فرض نقض شود، مدل ممکن است عملکرد بهینه نداشته باشد.
     - عدم کارایی در داده‌های با ابعاد بالا: برای مسائل با تعداد ویژگی‌های بسیار زیاد، رگرسیون لجستیک ممکن است دچار مشکل overfitting یا کارایی پایین شود.

5. **کاربردهای رگرسیون لجستیک**
   - **پزشکی:** پیش‌بینی بیماری‌ها یا احتمال بروز یک بیماری خاص با استفاده از ویژگی‌های فرد.
   - **بازاریابی:** پیش‌بینی اینکه آیا یک مشتری خرید می‌کند یا خیر (مدل‌سازی رفتار مشتری).
   - **تشخیص تقلب:** شناسایی تراکنش‌های تقلبی با استفاده از ویژگی‌های تراکنش.
   - **پرداخت‌سازی و تحلیل رضایت مشتری:** پیش‌بینی اینکه آیا یک مشتری از خدمات یا محصولات راضی است یا خیر.
   - **پردازش زبان طبیعی (NLP):** برای تحلیل احساسات (مثلاً تعیین اینکه آیا یک متن مثبت یا منفی است).

6. **نسخه‌های پیشرفته رگرسیون لجستیک**
   - **رگرسیون لجستیک چندکلاسه (Multinomial Logistic Regression):** در مسائل دسته‌بندی چندکلاسه (که بیش از دو کلاس وجود دارد)، رگرسیون لجستیک چندکلاسه استفاده می‌شود. این مدل یک نسخه گسترش‌یافته از رگرسیون لجستیک است که امکان پیش‌بینی بیش از دو کلاس را فراهم می‌آورد.
   - **رگرسیون لجستیک منظم‌شده (Regularized Logistic Regression):** در صورت وجود داده‌های زیادی با ویژگی‌های زیادی (high-dimensional data)، مدل ممکن است دچار overfitting شود. برای مقابله با این مشکل، از منظم‌سازی (Regularization) استفاده می‌شود. رایج‌ترین تکنیک‌های منظم‌سازی شامل L1 (Lasso) و L2 (Ridge) است.

# الگوریتم‌های یادگیری عمیق
## MLP (Perceptron چند لایه)
MLP یک نوع شبکه عصبی مصنوعی است که از چندین لایه پرسپترون برای یادگیری الگوهای پیچیده استفاده می‌کند.

## KAN (شبکه عصبی کانولوشنی)
شبکه عصبی کانولوشنی (Convolutional Neural Network - CNN) برای پردازش داده‌های تصویری و شناسایی الگوهای پیچیده استفاده می‌شود.

## Genetic MLP
Genetic MLP ترکیبی از الگوریتم‌های ژنتیک و شبکه‌های عصبی است که برای بهینه‌سازی ساختار و وزن‌های شبکه استفاده می‌شود.

# جمع‌بندی
در این تحقیق، الگوریتم‌های مختلف یادگیری ماشین و یادگیری عمیق برای پیش‌بینی بیماری‌های قلبی عروقی مورد بررسی قرار گرفتند. نتایج نشان داد که الگوریتم‌های یادگیری عمیق به دلیل توانایی در یادگیری الگوهای پیچیده، دقت بالاتری دارند.

# منابع
- مقاله CVD_paper
- منابع دیگر مرتبط با موضوع 